{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import regex as reg\n",
    "import os\n",
    "from pathlib import Path\n",
    "#NLP\n",
    "import spacy as sp\n",
    "from nltk.corpus import opinion_lexicon\n",
    "#sklearn\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#Mapping\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium import plugins\n",
    "\n",
    "# Import Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Process text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, subjectivity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "# Other models\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "# NLTK\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4783"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2006"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(opinion_lexicon.negative())\n",
    "len(opinion_lexicon.positive())\n",
    "#for w in range(len(opinion_lexicon.negative())):\n",
    "#              opinion_lexicon.negative()[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '..'+os.sep+'Tweets_5_11/'\n",
    "list_of_files = os.listdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = Path.home() \n",
    "#path = Path(filepath)\n",
    "#list_of_files = path / 'Tweets_5_11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.DataFrame()\n",
    "for p in list_of_files:\n",
    "    p = file_path+p\n",
    "    temp_df = pd.read_csv(p,usecols = ['tweet_id','text','region'])\n",
    "    tweet_df = tweet_df.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 130092 entries, 0 to 17\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   tweet_id  130092 non-null  object\n",
      " 1   text      130089 non-null  object\n",
      " 2   region    130086 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 4.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tweet_id    0\n",
       "text        3\n",
       "region      6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.info()\n",
    "tweet_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.drop_duplicates(subset = 'tweet_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 92325 entries, 0 to 17\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tweet_id  92325 non-null  object\n",
      " 1   text      92323 non-null  object\n",
      " 2   region    92321 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tweet_id    0\n",
       "text        2\n",
       "region      4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.info()\n",
    "tweet_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 92319 entries, 0 to 17\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tweet_id  92319 non-null  object\n",
      " 1   text      92319 non-null  object\n",
      " 2   region    92319 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tweet_id    0\n",
       "text        0\n",
       "region      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.dropna(inplace=True)\n",
    "tweet_df.info()\n",
    "tweet_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1246588222903214080</td>\n",
       "      <td>Shaukat Khanum Memorial Cancer Hospital offers...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1246587645779574784</td>\n",
       "      <td>Church congregants insisting on attending serv...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1246585369899892738</td>\n",
       "      <td>Rendering of a new 44-unit affordable housing ...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1246584511732695040</td>\n",
       "      <td>Double date, covid-style. pic.twitter.com/LWh2...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1246584386583068672</td>\n",
       "      <td>Si tuvieran que sacrificar un pueblo para acab...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1246584366471376896</td>\n",
       "      <td>El Covid-19 iba a llamarse Dolly Parton, pero ...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1246584353821315073</td>\n",
       "      <td>20-20  is perfect Vision but as a year 2020 is...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1246583763267514368</td>\n",
       "      <td>S/o to the homie @yerrrchubbs for being on liv...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1246583102534639617</td>\n",
       "      <td>Going on a road trip  fuck covid</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1246582098523115520</td>\n",
       "      <td>@greggutfeld We are given these numbers about ...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1246581635866415105</td>\n",
       "      <td>#covid-19 #coronavirus @ Long Beach, Californi...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1246581609559502849</td>\n",
       "      <td>Meanwhile, the president is saying he’s going ...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1246580557430976512</td>\n",
       "      <td>Serious question, if a model was used on #’s w...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1246580280313274368</td>\n",
       "      <td>@DonaldJTrumpJr I hope and pray your dad is no...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1246579907540529157</td>\n",
       "      <td>Nature’s brush strokes #sunset in a time of co...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index             tweet_id  \\\n",
       "0       0  1246588222903214080   \n",
       "1       1  1246587645779574784   \n",
       "2       2  1246585369899892738   \n",
       "3       3  1246584511732695040   \n",
       "4       4  1246584386583068672   \n",
       "5       5  1246584366471376896   \n",
       "6       6  1246584353821315073   \n",
       "7       7  1246583763267514368   \n",
       "8       8  1246583102534639617   \n",
       "9       9  1246582098523115520   \n",
       "10     10  1246581635866415105   \n",
       "11     11  1246581609559502849   \n",
       "12     12  1246580557430976512   \n",
       "13     13  1246580280313274368   \n",
       "14     14  1246579907540529157   \n",
       "\n",
       "                                                 text region  \n",
       "0   Shaukat Khanum Memorial Cancer Hospital offers...     LA  \n",
       "1   Church congregants insisting on attending serv...     LA  \n",
       "2   Rendering of a new 44-unit affordable housing ...     LA  \n",
       "3   Double date, covid-style. pic.twitter.com/LWh2...     LA  \n",
       "4   Si tuvieran que sacrificar un pueblo para acab...     LA  \n",
       "5   El Covid-19 iba a llamarse Dolly Parton, pero ...     LA  \n",
       "6   20-20  is perfect Vision but as a year 2020 is...     LA  \n",
       "7   S/o to the homie @yerrrchubbs for being on liv...     LA  \n",
       "8                    Going on a road trip  fuck covid     LA  \n",
       "9   @greggutfeld We are given these numbers about ...     LA  \n",
       "10  #covid-19 #coronavirus @ Long Beach, Californi...     LA  \n",
       "11  Meanwhile, the president is saying he’s going ...     LA  \n",
       "12  Serious question, if a model was used on #’s w...     LA  \n",
       "13  @DonaldJTrumpJr I hope and pray your dad is no...     LA  \n",
       "14  Nature’s brush strokes #sunset in a time of co...     LA  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = sp.load('en_core_web_sm')\n",
    "processed_tweet = []\n",
    "\n",
    "def clean_tweet(text):\n",
    "\n",
    "#replace anything that isn't a-z or A-Z with spaces including punctuations, exclamation etc\n",
    "#replace 'https://', 'www.' by space \n",
    "    ret_str = ' '\n",
    "    regex = r\"(.\\\\n|.\\\\t|.\\\\r)|([0-9]+)|([^\\w\\s])|(https{0,1}:\\/\\/\\S*)|(ww[wa-zA-Z0-9.com]+)|(pic.twitter\\S*)\"\n",
    "    ret_str = reg.sub(regex, '', str(text))\n",
    "    ret_str = ret_str.rstrip() \n",
    "    ret_str = ret_str.lstrip()\n",
    "    ret_str = ret_str.lower()\n",
    "    doc = spacy_nlp(ret_str)\n",
    "    all_lemmas =  [token.lemma_ for token in doc if  not token.is_stop \\\n",
    "                   and token.is_alpha and token.lemma_ != '-PRON-']\n",
    "    processed_tweet.append(\" \".join(all_lemmas)) # tweet cleaned up,tokenized.Copy each tweet as a string to the list\n",
    "    return all_lemmas #tweet cleaned up, tokenized return a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet():\n",
    "    clean_tweets = []\n",
    "    start = time.perf_counter()\n",
    "    counter = 0\n",
    "    for row in tweet_df['text']:\n",
    "        clean_tweets.append(clean_tweet(row))\n",
    "        counter += 1\n",
    "        \n",
    "    print(f\"Processed {counter} rows in training data\")\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    print(f\"Took { round((end-start)/60,0)} minutes to clean\")\n",
    "    return clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 93144 rows in training data\n",
      "Took 11.0 minutes to clean\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = process_tweet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3000 rows\n",
      "Processed 6000 rows\n",
      "Processed 9000 rows\n",
      "Processed 12000 rows\n",
      "Processed 15000 rows\n",
      "Processed 18000 rows\n",
      "Processed 21000 rows\n",
      "Processed 24000 rows\n",
      "Processed 27000 rows\n",
      "Processed 30000 rows\n",
      "Processed 33000 rows\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "sentiment = []\n",
    "start = time.perf_counter()\n",
    "\n",
    "for tokens in tweet_tokens:\n",
    "    positive_words = 0\n",
    "    negative_words = 0\n",
    "    count += 1\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in opinion_lexicon.positive():\n",
    "            positive_words += 1\n",
    "        elif tokens[i] in opinion_lexicon.negative():\n",
    "            negative_words += 1\n",
    "        \n",
    "    if positive_words > negative_words:\n",
    "        sentiment.append(1) #postive\n",
    "    elif positive_words < negative_words:\n",
    "        sentiment.append(-1) #negative\n",
    "    elif positive_words == negative_words:\n",
    "        sentiment.append(0) #neutral\n",
    "        \n",
    "    if count%10_000 == 0:\n",
    "        print(f\"Processed {count} rows\")\n",
    "            \n",
    "end = time.perf_counter()\n",
    "print(f\"Took { round((end-start)/60,0)} minutes for sentiment analysis\")          \n",
    "\n",
    "len(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['tweettokens'] = tweet_tokens\n",
    "tweet_df['processedtweet'] = processed_tweet\n",
    "tweet_df['sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 93144 entries, 0 to 17\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   tweet_id        93144 non-null  object\n",
      " 1   text            93144 non-null  object\n",
      " 2   region          93144 non-null  object\n",
      " 3   tweettokens     93144 non-null  object\n",
      " 4   processedtweet  93144 non-null  object\n",
      " 5   sentiment       93144 non-null  int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 5.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>region</th>\n",
       "      <th>tweettokens</th>\n",
       "      <th>processedtweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1246588222903214080</td>\n",
       "      <td>Shaukat Khanum Memorial Cancer Hospital offers...</td>\n",
       "      <td>LA</td>\n",
       "      <td>[shaukat, khanum, memorial, cancer, hospital, ...</td>\n",
       "      <td>shaukat khanum memorial cancer hospital offer ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1246587645779574784</td>\n",
       "      <td>Church congregants insisting on attending serv...</td>\n",
       "      <td>LA</td>\n",
       "      <td>[church, congregant, insist, attend, service, ...</td>\n",
       "      <td>church congregant insist attend service wake c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1246585369899892738</td>\n",
       "      <td>Rendering of a new 44-unit affordable housing ...</td>\n",
       "      <td>LA</td>\n",
       "      <td>[render, new, unit, affordable, housing, proje...</td>\n",
       "      <td>render new unit affordable housing project hav...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1246584511732695040</td>\n",
       "      <td>Double date, covid-style. pic.twitter.com/LWh2...</td>\n",
       "      <td>LA</td>\n",
       "      <td>[double, date, covidstyle]</td>\n",
       "      <td>double date covidstyle</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1246584386583068672</td>\n",
       "      <td>Si tuvieran que sacrificar un pueblo para acab...</td>\n",
       "      <td>LA</td>\n",
       "      <td>[si, tuvieran, que, sacrificar, un, pueblo, pa...</td>\n",
       "      <td>si tuvieran que sacrificar un pueblo para acab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1246588222903214080  Shaukat Khanum Memorial Cancer Hospital offers...   \n",
       "1  1246587645779574784  Church congregants insisting on attending serv...   \n",
       "2  1246585369899892738  Rendering of a new 44-unit affordable housing ...   \n",
       "3  1246584511732695040  Double date, covid-style. pic.twitter.com/LWh2...   \n",
       "4  1246584386583068672  Si tuvieran que sacrificar un pueblo para acab...   \n",
       "\n",
       "  region                                        tweettokens  \\\n",
       "0     LA  [shaukat, khanum, memorial, cancer, hospital, ...   \n",
       "1     LA  [church, congregant, insist, attend, service, ...   \n",
       "2     LA  [render, new, unit, affordable, housing, proje...   \n",
       "3     LA                         [double, date, covidstyle]   \n",
       "4     LA  [si, tuvieran, que, sacrificar, un, pueblo, pa...   \n",
       "\n",
       "                                      processedtweet  sentiment  \n",
       "0  shaukat khanum memorial cancer hospital offer ...          0  \n",
       "1  church congregant insist attend service wake c...          0  \n",
       "2  render new unit affordable housing project hav...         -1  \n",
       "3                             double date covidstyle          0  \n",
       "4  si tuvieran que sacrificar un pueblo para acab...          0  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.info()\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    0.374098\n",
       "-1    0.321910\n",
       " 1    0.303992\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df['sentiment'].value_counts(normalize = True)\n",
    "# 0 - neutral, 1 - positive, -1 - negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                             0\n",
       "tweet_id                                        1246588222903214080\n",
       "text              Shaukat Khanum Memorial Cancer Hospital offers...\n",
       "region                                                           LA\n",
       "tweettokens       [shaukat, khanum, memorial, cancer, hospital, ...\n",
       "processedtweet    shaukat khanum memorial cancer hospital offer ...\n",
       "sentiment                                                         0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweet_df['processedtweet']\n",
    "y = tweet_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62406,)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(30738,)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(30738,)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(62406,)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.33,random_state = 111)\n",
    "X_train.shape\n",
    "X_test.shape\n",
    "y_test.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(gsobject,xdata,ydata):\n",
    "    predicted = gsobject.predict(xdata)\n",
    "    actual_predicted = pd.DataFrame({\"Actual\" : ydata, \"Predicted\": predicted, 'Text': xdata})\n",
    "    return actual_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs_type = grid search object\n",
    "#xtraindata,xtestdata = xtrain and xtest \n",
    "#ytraindata,ytest = ytrain and ytest\n",
    "def fit_grid_search(gs_type,xtraindata,ytraindata,xtestdata,ytestdata):\n",
    "    start = time.perf_counter()\n",
    "    gs_type.fit(xtraindata,ytraindata)\n",
    "    print(f\"Grid search accuracy for estimator \\033[1m{gs_type.estimator.steps[1][0]}\\033[0m \"\n",
    "          f\"transformer \\033[1m{gs_type.estimator.steps[0][0]}\\033[0m : \\n\")\n",
    "    print(f\"On training data is {gs_type.score(xtraindata,ytraindata)}\")\n",
    "    print(f\"On test data is {gs_type.score(xtestdata,ytestdata)}\")\n",
    "    print(f\"Grid search best score (avg of cv scores) {gs_type.best_score_}\\n\\n\")\n",
    "    print(f\"Model with best fitting parameter is \\n {gs_type.best_estimator_.get_params}\\n\\n\")\n",
    "    #print(gs_type.best_estimator_.named_steps['logisticregression'].coef_)\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Took \\033[1m{ round((end-start)/60,0)}\\033[0m minutes to complete\")\n",
    "    cv_results_df = pd.DataFrame(gs_type.cv_results_)\n",
    "    return cv_results_df,gs_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipette_tfidf = make_pipeline(TfidfVectorizer(),LogisticRegression())\n",
    "pipe_params_tfidf = {\n",
    "    'tfidfvectorizer__max_features': [15_000,25_000,30_000],\n",
    "    'tfidfvectorizer__ngram_range': [ (1,2),(1,3)],\n",
    "    'tfidfvectorizer__min_df' : [5,10], #discard words that appear in less than 5 or 10 documents\n",
    "    'tfidfvectorizer__max_df' : [0.80], #exclude words that cross this threshold(how many documents contained a term)\n",
    "    'tfidfvectorizer__stop_words' : [None],\n",
    "    'logisticregression__max_iter': [1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfidf = GridSearchCV(pipette_tfidf, \n",
    "                  pipe_params_tfidf, \n",
    "                  cv = 5 ,\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search accuracy for estimator \u001b[1mlogisticregression\u001b[0m transformer \u001b[1mtfidfvectorizer\u001b[0m : \n",
      "\n",
      "On training data is 0.8896900939012274\n",
      "On test data is 0.8263062007938057\n",
      "Grid search best score (avg of cv scores) 0.8162034929529354\n",
      "\n",
      "\n",
      "Model with best fitting parameter is \n",
      " <bound method Pipeline.get_params of Pipeline(memory=None,\n",
      "         steps=[('tfidfvectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.8, max_features=15000,\n",
      "                                 min_df=10, ngram_range=(1, 2), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=2000,\n",
      "                                    multi_class='auto', n_jobs=None,\n",
      "                                    penalty='l2', random_state=None,\n",
      "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)>\n",
      "\n",
      "\n",
      "Took \u001b[1m8.0\u001b[0m minutes to complete\n"
     ]
    }
   ],
   "source": [
    "results_tfidf,gs_tfidf = fit_grid_search(gs_tfidf,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTraining set data\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>quarantine jam groove friend dvibesonda denver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3357</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>feel like helpful distinction maybe harmful ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>austrian rail company look worker shorttime wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5677</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>imply premise original question covid overcod ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>open prematurely shall place emergency icu ppe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5368</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>come people overdose ailment san francisco goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6020</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>covid pandemic crisis connected mediafrenzy hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>tx share need reduce documentation burden set ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>easily big benefactor corona virus guy have pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>day be ironic life gift enjoy day tomorrow cov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6884 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual  Predicted                                               Text\n",
       "2810      -1          1  quarantine jam groove friend dvibesonda denver...\n",
       "3357       1         -1  feel like helpful distinction maybe harmful ac...\n",
       "235        0          1  austrian rail company look worker shorttime wo...\n",
       "5677       0         -1  imply premise original question covid overcod ...\n",
       "1510       0          1  open prematurely shall place emergency icu ppe...\n",
       "...      ...        ...                                                ...\n",
       "5368       0          1  come people overdose ailment san francisco goo...\n",
       "6020       0         -1  covid pandemic crisis connected mediafrenzy hi...\n",
       "127        0          1  tx share need reduce documentation burden set ...\n",
       "3451       0         -1  easily big benefactor corona virus guy have pe...\n",
       "2283       0          1  day be ironic life gift enjoy day tomorrow cov...\n",
       "\n",
       "[6884 rows x 3 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTesting set data\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>great help boil census want student count covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>coronavirus newsom say californian expect infe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>good chance catch corona virus feeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8600</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>confirm case covid find america trump virus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4403</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mood bored star rona wonder long s go to s lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>corona update have get mad compliment face mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>mood day quarantine lockdown passthe corona vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12601</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>task california firefighter difficult covid ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7723</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>confirmen si todo estábamos en la mejor etapa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>snaplatt conspiracy theory run dmorey tweet oc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5339 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Predicted                                               Text\n",
       "2299        0          1    great help boil census want student count covid\n",
       "2035        1          0  coronavirus newsom say californian expect infe...\n",
       "3330        0         -1             good chance catch corona virus feeling\n",
       "8600        0         -1        confirm case covid find america trump virus\n",
       "4403        0          1  mood bored star rona wonder long s go to s lon...\n",
       "...       ...        ...                                                ...\n",
       "2753        1          0  corona update have get mad compliment face mas...\n",
       "4949       -1          0  mood day quarantine lockdown passthe corona vi...\n",
       "12601      -1          0  task california firefighter difficult covid ne...\n",
       "7723        1          0  confirmen si todo estábamos en la mejor etapa ...\n",
       "1474        0         -1  snaplatt conspiracy theory run dmorey tweet oc...\n",
       "\n",
       "[5339 rows x 3 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\\033[1mTraining set data\\033[0m\")\n",
    "actual_v_predicted_train= conf_matrix(gs_tfidf,X_train,y_train)\n",
    "\n",
    "mask = actual_v_predicted_train[(actual_v_predicted_train['Actual']) != (actual_v_predicted_train['Predicted'])]\n",
    "mask\n",
    "\n",
    "print(f\"\\033[1mTesting set data\\033[0m\")\n",
    "actual_v_predicted_test = conf_matrix(gs_tfidf,X_test,y_test)\n",
    "\n",
    "mask = actual_v_predicted_test[(actual_v_predicted_test['Actual']) != (actual_v_predicted_test['Predicted'])]\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipette_cvect = make_pipeline(CountVectorizer(),LogisticRegression())\n",
    "pipe_params_cvect = {\n",
    "    'countvectorizer__max_features': [15_000,25_000,30_000],\n",
    "    'countvectorizer__ngram_range':  [(1,2),(1,3)],\n",
    "    'countvectorizer__min_df' : [5,10], #discard words that appear in less than 5 or 10 documents\n",
    "    'countvectorizer__max_df' : [0.8], #exclude words that cross this threshold(how many docs contained a term)\n",
    "    'countvectorizer__stop_words' : [None],\n",
    "    'logisticregression__max_iter': [1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cvect = GridSearchCV(pipette_cvect, \n",
    "                  pipe_params_cvect, \n",
    "                  cv = 5,\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  9.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search accuracy for estimator \u001b[1mlogisticregression\u001b[0m transformer \u001b[1mcountvectorizer\u001b[0m : \n",
      "\n",
      "On training data is 0.9624875813223087\n",
      "On test data is 0.876179322011842\n",
      "Grid search best score (avg of cv scores) 0.860285807155606\n",
      "\n",
      "\n",
      "Model with best fitting parameter is \n",
      " <bound method Pipeline.get_params of Pipeline(memory=None,\n",
      "         steps=[('countvectorizer',\n",
      "                 CountVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
      "                                 input='content', lowercase=True, max_df=0.8,\n",
      "                                 max_features=15000, min_df=10,\n",
      "                                 ngram_range=(1, 2), preprocessor=None,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, vocabulary=None)),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=1000,\n",
      "                                    multi_class='auto', n_jobs=None,\n",
      "                                    penalty='l2', random_state=None,\n",
      "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)>\n",
      "\n",
      "\n",
      "Took \u001b[1m9.0\u001b[0m minutes to complete\n"
     ]
    }
   ],
   "source": [
    "results_cvect,gs_cvect = fit_grid_search(gs_cvect,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTraining set data\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>quarantine jam groove friend dvibesonda denver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7542</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>break live magic covid quarantinelife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7218</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>rain la not gym covid wait today perfectstorm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>idea wish not consistency base explain go fact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>rhetoric like carville not help s true express...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14281</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>appropriate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6936</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>see celebrity etc facetime etc home be realize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>coincidentally success rate sbs generally unaf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5329</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>fact asian american asian immigrant deserve th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>easily big benefactor corona virus guy have pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2341 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Predicted                                               Text\n",
       "2810       -1          0  quarantine jam groove friend dvibesonda denver...\n",
       "7542        0         -1              break live magic covid quarantinelife\n",
       "7218        0         -1  rain la not gym covid wait today perfectstorm ...\n",
       "9917       -1          0  idea wish not consistency base explain go fact...\n",
       "492        -1          0  rhetoric like carville not help s true express...\n",
       "...       ...        ...                                                ...\n",
       "14281       1          0                                        appropriate\n",
       "6936       -1          0  see celebrity etc facetime etc home be realize...\n",
       "2749        0          1  coincidentally success rate sbs generally unaf...\n",
       "5329        0         -1  fact asian american asian immigrant deserve th...\n",
       "3451        0         -1  easily big benefactor corona virus guy have pe...\n",
       "\n",
       "[2341 rows x 3 columns]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTesting set data\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>great help boil census want student count covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4403</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mood bored star rona wonder long s go to s lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4710</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>be walk dog neighbor start walk close quaranti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8009</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ios update iphone set include new software dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8814</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>like airmail covid awful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6939</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>dear speakerpelosi impeach donald trump type c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>patience wear thin sixfeetapart quarantine qua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>corona update have get mad compliment face mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12601</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>task california firefighter difficult covid ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unpopular opinion people make quarantine read ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3806 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Predicted                                               Text\n",
       "2299        0          1    great help boil census want student count covid\n",
       "4403        0          1  mood bored star rona wonder long s go to s lon...\n",
       "4710        1          0  be walk dog neighbor start walk close quaranti...\n",
       "8009        1          0  ios update iphone set include new software dev...\n",
       "8814        0          1                           like airmail covid awful\n",
       "...       ...        ...                                                ...\n",
       "6939       -1          1  dear speakerpelosi impeach donald trump type c...\n",
       "2364        1          0  patience wear thin sixfeetapart quarantine qua...\n",
       "2753        1          0  corona update have get mad compliment face mas...\n",
       "12601      -1          0  task california firefighter difficult covid ne...\n",
       "3272       -1          0  unpopular opinion people make quarantine read ...\n",
       "\n",
       "[3806 rows x 3 columns]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\\033[1mTraining set data\\033[0m\")\n",
    "actual_v_predicted_train= conf_matrix(gs_cvect,X_train,y_train)\n",
    "\n",
    "mask = actual_v_predicted_train[(actual_v_predicted_train['Actual']) != (actual_v_predicted_train['Predicted'])]\n",
    "mask\n",
    "\n",
    "print(f\"\\033[1mTesting set data\\033[0m\")\n",
    "actual_v_predicted_test = conf_matrix(gs_cvect,X_test,y_test)\n",
    "\n",
    "mask = actual_v_predicted_test[(actual_v_predicted_test['Actual']) != (actual_v_predicted_test['Predicted'])]\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tweets\n",
    "tweet_la_1 = pd.read_csv('../Tweets_5_11/tweet_LA_3_12_23.csv')\n",
    "tweet_la_2 = pd.read_csv('../Tweets_5_11/tweet_LA_3_1_12.csv')\n",
    "tweet_la_3 = pd.read_csv('../Tweets_5_11/tweet_LA_3_23_31.csv')\n",
    "tweet_la_4 = pd.read_csv('../Tweets_5_11/tweet_LA_4_12_23.csv')\n",
    "tweet_la_5 = pd.read_csv('../Tweets_5_11/tweet_LA_4_1_12.csv')\n",
    "tweet_la_6 = pd.read_csv('../Tweets_5_11/tweet_LA_4_23_30.csv')\n",
    "tweet_la_7 = pd.read_csv('../Tweets_5_11/tweet_LA_5_12_23.csv')\n",
    "tweet_la_8 = pd.read_csv('../Tweets_5_11/tweet_LA_5_1_12.csv')\n",
    "\n",
    "tweet_sf_1 = pd.read_csv('../Tweets_5_11/tweet_SF_3_12_23.csv')\n",
    "tweet_sf_2 = pd.read_csv('../Tweets_5_11/tweet_SF_3_1_12.csv')\n",
    "tweet_sf_3 = pd.read_csv('../Tweets_5_11/tweet_SF_3_23_31.csv')\n",
    "tweet_sf_4 = pd.read_csv('../Tweets_5_11/tweet_SF_4_12_23.csv')\n",
    "tweet_sf_5 = pd.read_csv('../Tweets_5_11/tweet_SF_4_1_12.csv')\n",
    "tweet_sf_6 = pd.read_csv('../Tweets_5_11/tweet_SF_4_23_30.csv')\n",
    "tweet_sf_7 = pd.read_csv('../Tweets_5_11/tweet_SF_5_12_23.csv')\n",
    "tweet_sf_8 = pd.read_csv('../Tweets_5_11/tweet_SF_5_1_12.csv')\n",
    "\n",
    "tweet_bk_1 = pd.read_csv('../Tweets_5_11/tweet_BAKERSFIELD_3_12_23.csv')\n",
    "tweet_bk_2 = pd.read_csv('../Tweets_5_11/tweet_BAKERSFIELD_3_1_12.csv')\n",
    "tweet_bk_3 = pd.read_csv('../Tweets_5_11/tweet_BAKERSFIELD_3_23_31.csv')\n",
    "tweet_bk_4 = pd.read_csv('../Tweets_5_11/tweet_BAKERSFIELD_4_12_23.csv')\n",
    "tweet_bk_5 = pd.read_csv('../Tweets_5_11/tweet_BAKERSFIELD_4_1_12.csv')\n",
    "tweet_bk_6 = pd.read_csv('../Tweets_5_11/tweet_BAKERSFIELD_4_23_30.csv')\n",
    "tweet_bk_7 = pd.read_csv('../Tweets_5_11/tweet_BAKERSFIELD_5_12_23.csv')\n",
    "tweet_bk_8 = pd.read_csv('../Tweets_5_11/tweet_BAKERSFIELD_5_1_12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate tweets\n",
    "tweets = pd.concat([tweet_la_1, tweet_la_2, tweet_la_3, tweet_la_4, tweet_la_5, tweet_la_6, tweet_la_7, tweet_la_8,\n",
    "                    tweet_sf_1, tweet_sf_2, tweet_sf_3, tweet_sf_4, tweet_sf_5, tweet_sf_6, tweet_sf_7, tweet_sf_8,\n",
    "                    tweet_bk_1, tweet_bk_2, tweet_bk_3, tweet_bk_4, tweet_bk_5, tweet_bk_6, tweet_bk_7, tweet_bk_8])\n",
    "# drop duplicates\n",
    "tweets.drop_duplicates(inplace = True)\n",
    "\n",
    "# reset index\n",
    "tweets.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(columns = 'text_html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116872, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id       0\n",
       "username       1\n",
       "text           2\n",
       "tweet_date     2\n",
       "hashtag        2\n",
       "tweet_url      2\n",
       "search_term    2\n",
       "lat            2\n",
       "long           5\n",
       "region         5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check nulls\n",
    "tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing nulls since there aren't many\n",
    "tweets.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116864, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "\n",
    "def process_text(X):\n",
    "    X_clean = []    \n",
    "    counter = 0\n",
    "\n",
    "    for train in X:\n",
    "        X_clean.append(clean_text(train))\n",
    "        counter += 1\n",
    "    print(f\"Processed {counter} rows in data\")\n",
    "\n",
    "    return X_clean\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "#replace anything that isn't a-z or A-Z with spaces including punctuations, exclamation etc\n",
    "#replace 'https://', 'www.' by space \n",
    "    ret_str = ' '\n",
    "    regex = r\"(.\\\\n|.\\\\t|.\\\\r)|([0-9]+)|([^\\w\\s])|(https{0,1}:\\/\\/\\S*)|(ww[wa-zA-Z0-9.com]+)\"\n",
    "    ret_str = reg.sub(regex, '', str(text))\n",
    "    ret_str = ret_str.rstrip() \n",
    "    ret_str = ret_str.lstrip()\n",
    "    ret_str = ret_str.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    doc = tokenizer.tokenize(ret_str)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    all_lemmas =  [lemmatizer.lemmatize(token) for token in doc]\n",
    "    return(\" \".join(all_lemmas)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 116864 rows in data\n"
     ]
    }
   ],
   "source": [
    "tweets['clean_text'] = process_text(tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COVID backwards is DIVOC and what in DIVOC is ...</td>\n",
       "      <td>covid backwards is divoc and what in divoc is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good job there on 1600 Pennsylvania Avenue.Tel...</td>\n",
       "      <td>good job there on pennsylvania avenuetelling p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...unsolicited advice for stressed out univers...</td>\n",
       "      <td>unsolicited advice for stressed out university...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is SARS-CoV- 2 not COVID 19</td>\n",
       "      <td>it is sarscov not covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If I get covid it’s 100% my roomies fault. My ...</td>\n",
       "      <td>if i get covid it my roomy fault my life is al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  COVID backwards is DIVOC and what in DIVOC is ...   \n",
       "1  Good job there on 1600 Pennsylvania Avenue.Tel...   \n",
       "2  ...unsolicited advice for stressed out univers...   \n",
       "3                     It is SARS-CoV- 2 not COVID 19   \n",
       "4  If I get covid it’s 100% my roomies fault. My ...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  covid backwards is divoc and what in divoc is ...  \n",
       "1  good job there on pennsylvania avenuetelling p...  \n",
       "2  unsolicited advice for stressed out university...  \n",
       "3                            it is sarscov not covid  \n",
       "4  if i get covid it my roomy fault my life is al...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking original vs clean text\n",
    "tweets[['text', 'clean_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting sentiment\n",
    "\n",
    "clean_text = tweets['clean_text'].values.tolist()\n",
    "\n",
    "from textblob import TextBlob\n",
    "sentiment =[]\n",
    "\n",
    "for i in range(len(clean_text)):\n",
    "    sentiment.append(TextBlob(clean_text[i]).sentiment.polarity)\n",
    "\n",
    "tweets['sentiment_score'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting sentiment\n",
    "\n",
    "clean_text = tweets['clean_text'].values.tolist()\n",
    "\n",
    "from textblob import TextBlob\n",
    "sentiment =[]\n",
    "\n",
    "for i in range(len(clean_text)):\n",
    "    sentiment.append(TextBlob(clean_text[i]).sentiment.polarity)\n",
    "\n",
    "tweets['sentiment_score'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sentiment'] = tweets['sentiment_score'].apply(lambda x: 1 if x > 0 \n",
    "                                                      else -1 if x < 0\n",
    "                                                      else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    26329\n",
       " 0    19971\n",
       "-1    12640\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[tweets['region'] == 'LA']['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    23146\n",
       " 0    17684\n",
       "-1    10627\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[tweets['region'] == 'SF']['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    2686\n",
       " 0    2265\n",
       "-1    1516\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[tweets['region'] == 'BAKERSFIELD']['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting sentiment\n",
    "\n",
    "def grid_search_reddit_lr(X_train, X_test, y_train, y_test, vectorizer = TfidfVectorizer(), ngram = [(1,1), (2,2), (1,2)],\n",
    "                       stop_w = ['english'], penalty = ['none', 'l2'], c = [0.2,0.5, 1,2,3], m_iter = [1000], max_d = [1.0], min_d = [1], max_f = [None], accents = [None]):\n",
    "    \n",
    "    # Define Pipeline - Remeber the step structure (list of tuple)\n",
    "    pipe = Pipeline(steps = [('vectorizer', vectorizer),\n",
    "                             ('model', LogisticRegression())\n",
    "                              ])\n",
    "\n",
    "    # Construct Grid Parameters\n",
    "    hyperparams = {'vectorizer__ngram_range': ngram,\n",
    "                   'vectorizer__stop_words': stop_w,\n",
    "                   'vectorizer__max_df': max_d,\n",
    "                   'vectorizer__min_df': min_d,\n",
    "                   'vectorizer__max_features': max_f,\n",
    "                   'vectorizer__strip_accents': accents,\n",
    "                   'model__penalty': penalty,\n",
    "                   'model__C': c,\n",
    "                   'model__max_iter': m_iter\n",
    "                  }\n",
    "\n",
    "    # Perform Grid Search\n",
    "    lr = GridSearchCV(pipe,\n",
    "                     param_grid = hyperparams,\n",
    "                     cv = 3,\n",
    "                     scoring = 'accuracy')\n",
    "\n",
    "    results = lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Get score\n",
    "    train_score = results.best_score_\n",
    "    print('Best crossval score: {:.4f}'.format(train_score))\n",
    "    train_score = results.score(X_train, y_train)\n",
    "    print('Best TRAIN score: {:.4f}'.format(train_score))\n",
    "    test_score = results.score(X_test, y_test)\n",
    "    print('Best TEST score: {:.4f}'.format(test_score))\n",
    "    print(results.best_params_)\n",
    "\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87648,)\n",
      "(87648,)\n"
     ]
    }
   ],
   "source": [
    "X = tweets['clean_text']\n",
    "y = tweets['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RoxanaRuvalcaba/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/RoxanaRuvalcaba/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/RoxanaRuvalcaba/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/RoxanaRuvalcaba/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/RoxanaRuvalcaba/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best crossval score: 0.8661\n",
      "Best TRAIN score: 0.9769\n",
      "Best TEST score: 0.8852\n",
      "{'model__C': 10.0, 'model__max_iter': 1000, 'model__penalty': 'l2', 'vectorizer__max_df': 1.0, 'vectorizer__max_features': None, 'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 1), 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': None}\n"
     ]
    }
   ],
   "source": [
    "lr = grid_search_reddit_lr(X_train, X_test, y_train, y_test, vectorizer = TfidfVectorizer(), penalty = ['l2'], \n",
    "                           max_f = [None], c = np.logspace(-3,3,7), stop_w = ['english'], ngram = [(1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7017</th>\n",
       "      <td>best</td>\n",
       "      <td>-11.754240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32910</th>\n",
       "      <td>great</td>\n",
       "      <td>-8.712650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45586</th>\n",
       "      <td>lol</td>\n",
       "      <td>-8.210368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31881</th>\n",
       "      <td>glad</td>\n",
       "      <td>-6.845907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33977</th>\n",
       "      <td>happy</td>\n",
       "      <td>-6.494437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18113</th>\n",
       "      <td>crazy</td>\n",
       "      <td>13.761291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81104</th>\n",
       "      <td>stupid</td>\n",
       "      <td>14.413171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92853</th>\n",
       "      <td>worst</td>\n",
       "      <td>15.212444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5476</th>\n",
       "      <td>bad</td>\n",
       "      <td>15.432116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76976</th>\n",
       "      <td>sick</td>\n",
       "      <td>18.297016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95597 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       labels       coef\n",
       "7017     best -11.754240\n",
       "32910   great  -8.712650\n",
       "45586     lol  -8.210368\n",
       "31881    glad  -6.845907\n",
       "33977   happy  -6.494437\n",
       "...       ...        ...\n",
       "18113   crazy  13.761291\n",
       "81104  stupid  14.413171\n",
       "92853   worst  15.212444\n",
       "5476      bad  15.432116\n",
       "76976    sick  18.297016\n",
       "\n",
       "[95597 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'labels':lr.best_estimator_[0].get_feature_names(), 'coef':lr.best_estimator_[1].coef_[0]} ).sort_values('coef')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
